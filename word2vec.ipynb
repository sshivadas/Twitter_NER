{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149a01b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Load the YAML config\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32a2f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b606299",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = config[\"data\"][\"train_path\"]\n",
    "test_path = config[\"data\"][\"test_path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d795bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokens(tokens, keep_mentions=False):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    cleaned = []\n",
    "\n",
    "    for token in tokens:\n",
    "        token_lower = token.lower()\n",
    "\n",
    "        # Optional: keep mentions like @Google\n",
    "        if token.startswith('@') and keep_mentions:\n",
    "            cleaned.append(token_lower)\n",
    "            continue\n",
    "        elif token.startswith('@'):\n",
    "            continue\n",
    "\n",
    "        # Remove URLs\n",
    "        if re.match(r'http\\S+', token):\n",
    "            continue\n",
    "\n",
    "        # Remove hashtags (keep word only)\n",
    "        if token.startswith('#'):\n",
    "            token = token[1:]\n",
    "\n",
    "        # Remove punctuation-only tokens\n",
    "        if re.match(r'^[\\W_]+$', token):\n",
    "            continue\n",
    "\n",
    "        # Remove stopwords\n",
    "        if token_lower in stop_words:\n",
    "            continue\n",
    "\n",
    "        cleaned.append(token_lower)\n",
    "    \n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039b03c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_conll_sentences(filepath):\n",
    "    sentences = []\n",
    "    tokens = []\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if tokens:\n",
    "                    sentences.append(tokens)\n",
    "                    tokens = []\n",
    "            else:\n",
    "                parts = line.split()\n",
    "                if len(parts) == 2:\n",
    "                    token, _ = parts  # We skip the tag for Gensim\n",
    "                    tokens.append(token)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d75e287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_gensim_sentences(sentences, keep_mentions=False):\n",
    "    return [clean_tokens(sentence, keep_mentions=keep_mentions) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73318abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = config[\"data\"][\"train_path\"]\n",
    "test_path = config[\"data\"][\"test_path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94ec1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sentences = extract_conll_sentences(train_path)\n",
    "gensim_sentences = prepare_gensim_sentences(raw_sentences, keep_mentions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb880c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = set(word for sentence in gensim_sentences for word in sentence)\n",
    "vocab_size = len(unique_words)\n",
    "print(f\"Number of unique words: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cea9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the model\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=gensim_sentences,\n",
    "    vector_size=7236,     # Size of word embeddings\n",
    "    window=5,            # Context window size\n",
    "    min_count=2,         # Ignore words with frequency < 2\n",
    "    workers=4,           # Use 4 threads for training\n",
    "    sg=1                 # 1 for skip-gram, 0 for CBOW\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e0f64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to disk\n",
    "w2v_model.save(\"models/twitter_word2vec.model\")\n",
    "\n",
    "# Later load it\n",
    "w2v_model = Word2Vec.load(\"models/twitter_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ea5e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(w2v_model .wv.index_to_key)  # vocabulary words\n",
    "word_vectors = w2v_model .wv[words]       # corresponding embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0051c812",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 10  # You can experiment with 5â€“15\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a6385c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "word_vectors_2d = tsne.fit_transform(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca4f09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "colors = plt.cm.tab10(cluster_labels / max(cluster_labels))  # assign color to cluster\n",
    "\n",
    "for i, (x, y) in enumerate(word_vectors_2d):\n",
    "    plt.scatter(x, y, color=colors[i])\n",
    "    plt.text(x + 0.001, y + 0.001, words[i], fontsize=9)\n",
    "\n",
    "plt.title(\"Word2Vec Embedding Clusters (KMeans + t-SNE)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eee6d2",
   "metadata": {},
   "source": [
    "1. Clearly Separated color Bands\n",
    "Distinct curved â€œbandsâ€ â€” likely due to how t-SNE lays out high-dimensional clusters.\n",
    "\n",
    "This means clusters are semantically strong â€” KMeans found meaningful groupings.\n",
    "\n",
    "2. Dense Regions\n",
    "Certain zones are densely packed with words, indicating:\n",
    "\n",
    "Very frequent/common tokens.\n",
    "\n",
    "Words that are used in similar tweet contexts (e.g., greetings, time references, sentiments).\n",
    "\n",
    "3. 10 Color-coded Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941d8d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "cluster_to_words = defaultdict(list)\n",
    "for word, label in zip(words, cluster_labels):\n",
    "    cluster_to_words[label].append(word)\n",
    "\n",
    "for cluster_id, word_list in cluster_to_words.items():\n",
    "    print(f\"Cluster {cluster_id}: {word_list}\")  # Top 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ac6e5a",
   "metadata": {},
   "source": [
    "Cluster\tSample Words\tLabel\n",
    "Clusterâ€¯2\trt, day, today, tonight, week, years, get\t            ðŸŒ Time & Common Twitter Terms â€“ includes temporal references and very      frequent tokens\n",
    "\n",
    "Clusterâ€¯6\ttomorrow, friday, lol, good, love, sunday, school\tðŸ—“ï¸ Weekly Time + Sentiment â€“ days of the week mixed with positive or casual expressions\n",
    "\n",
    "Clusterâ€¯4\thappy, game, birthday, excited, thanks, saturday, music\tðŸŽ‰ Celebration/Social Events â€“ words linked to happy events, celebrations, or social activities\n",
    "\n",
    "Clusterâ€¯1\tcoming, morning, awesome, yesterday, omg, miss, sorry\tðŸ˜ Emotion + Time â€“ a mix of temporal words with emotional tone\n",
    "\n",
    "Clusterâ€¯5\twin, tired, lmao, damn, love, movie, london\tðŸŽ­ Entertainment & Reactions â€“ humor, emotion, places, and pop culture\n",
    "\n",
    "Clusterâ€¯8\tfootball, festival, gym, family, event, pregnant, doctor, park, jfk\tðŸ™ï¸ Activities & Locations â€“ events, health, and places like jfk airport\n",
    "\n",
    "Clusterâ€¯3\tgoin, song, goodnight, girl, york, peace, drunk, nyc\tðŸŒ† Nightlife & Places â€“ informal speech, partying, and urban locations\n",
    "\n",
    "Clusterâ€¯9\tsweet, hurt, shopping, sushi, paris, winter, country, miami, anniversary\tðŸŒ Feelings & Travel â€“ emotions, food, travel destinations, and special occasions\n",
    "\n",
    "Clusterâ€¯0\tcoffee, canada, phoenix, angry, sunny, reuters, media\tðŸ“Œ General Daily Life & Media â€“ news sources, weather, sentiment, and locations\n",
    "\n",
    "Clusterâ€¯7\tvacation, fire, style, south, jets, vancouver, deadline, england\tâœˆï¸ Travel & Lifestyle â€“ destinations, time phrases, sports, and lifestyle events\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bfc230",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸš« Inferences of Word2Vec model:\n",
    "\n",
    "1. **Lack of Context Awareness**\n",
    "\n",
    "   * Word2Vec generates a **single static vector per word**, regardless of its usage.\n",
    "   * Words like `\"game\"`, `\"light\"`, `\"cold\"` have multiple meanings but only one embedding â†’ leads to **semantic overlap** in clusters.\n",
    "\n",
    "2. **Overlapping and Ambiguous Clusters**\n",
    "\n",
    "   * Many clusters contain words that fit in **multiple topics**, causing **unclear or fuzzy boundaries**.\n",
    "   * Emotion words (`love`, `miss`, `today`) appear in several clusters.\n",
    "\n",
    "3. **Dominance of High-Frequency Words**\n",
    "\n",
    "   * Frequent and generic words (e.g., `today`, `good`, `love`) **dominate cluster centers**, overshadowing more meaningful but rarer terms.\n",
    "\n",
    "4. **Clusters Not Easily Interpretable**\n",
    "\n",
    "   * Clusters don't map cleanly to distinct themes (e.g., emotion, location, activity).\n",
    "   * Difficult to assign **human-readable labels** due to semantic mixing.\n",
    "\n",
    "5. **Ignores Word Order and Syntax**\n",
    "\n",
    "   * Word2Vec doesnâ€™t capture **phrases or grammar**, so `\"cold war\"` and `\"cold drink\"` are treated similarly to just `\"cold\"`.\n",
    "\n",
    "6. **Word-Level Clustering Limitation**\n",
    "\n",
    "   * Clustering individual words rather than full tweets or phrases **ignores full message context**.\n",
    "   * This reduces the model's utility in content or user-level insights.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ’¡ Better Alternatives (Briefly):\n",
    "\n",
    "* **Contextual embeddings** (BERT, RoBERTa): Capture word meaning based on sentence.\n",
    "* **Sentence-level clustering**: Cluster entire tweets, not just words.\n",
    "* **HDBSCAN + UMAP**: Better separation and visualization of clusters.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
